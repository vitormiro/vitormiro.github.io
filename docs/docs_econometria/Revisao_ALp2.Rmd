---
title: "Revisão de Álgebra Linear (2)"
subtitle: "Econometria I"
author: "Vitor Hugo Miro"
output: html_notebook
---

## 1. Sistemas Lineares

### Sistemas de equações lineares

Um sistema de $n$ equações simultâneas em $k$ variáveis é dado por:

$$
\begin{align*}
    a_{11} x_1 + a_{12} x_2 + \cdots + a_{1k} x_k &= b_1  \\
    a_{12} x_1 + a_{22} x_2 + \cdots + a_{2k} x_k &= b_2  \\
        &\vdots \\
    a_{n1} x_1 + a_{n2} x_2 + \cdots + a_{nk} x_k &= b_n  \\
\end{align*}
$$

e pode ser expresso na forma matricial como

$$\mathbf{Ax} = \mathbf{b}$$

, onde $\mathbf{A}$ é uma matriz $n \times k$ de coeficientes $[a_{ij}]$, $\mathbf{x}$ é um vetor coluna das variáveis $x_1, \dots, x_K$, e $\mathbf{b}$ é o vetor coluna de constantes $b_1, \dots, b_n$.

Usando a notação matricial, podemos escrever:

$$
\begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1k} \\
    a_{21} & a_{22} & \cdots & a_{2k} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nk}
\end{bmatrix} 
\begin{bmatrix}
    x_1 \\
    x_2 \\
    \vdots \\
    x_k
\end{bmatrix} = 
\begin{bmatrix}
    b_1 \\
    b_2 \\
    \vdots \\
    b_n
\end{bmatrix}
$$

A solução deste sitema é dada por:

$$\mathbf{x} = \mathbf{A}^{-1}\mathbf{b}$$

**EXEMPLO**

Considere o seguinte sistema de equações:

$$
\begin{align*}
x_1 + x_2 + x_3 &= 5 \\[2ex]
2x_1 – x_2 + 6x_3 &= 12 \\[2ex]
x_1 + 3x_2 + 5x_3 &= 17
\end{align*}
$$

Na forma matricial:

$$
\begin{align*} 
\mathbf{Ax} &= \mathbf{b} \\
\begin{bmatrix}
1 & 1 & 1 \\
2 & – 1 & 6\\
1 & 3 & 5
\end{bmatrix} 
\begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix} &= 
\begin{bmatrix}
5 \\ 12 \\ 17
\end{bmatrix}
\end{align*}
$$

Para resolver:

$$
\begin{split}
\mathbf{x} &= \mathbf{A}^{-1}\mathbf{b}\\
\begin{bmatrix}
x_1 \\
x_2\\
x_3
\end{bmatrix} &= \begin{bmatrix}
1 & 1 & 1 \\
2 & – 1 & 6\\
1 & 3 & 5
\end{bmatrix}^{-1} \begin{bmatrix}
5 \\
12\\
17
\end{bmatrix}
\end{split}
$$

No `R` podemos criar as matrizes $\mathbf{A}$ e $\mathbf{b}$:

```{r}
# Matriz A
A = matrix(c(1, 1, 1, 2, -1, 6, 1, 3, 5), 
           byrow = TRUE, 
           ncol = 3)
cat("A=\n")
print(A)

# Matriz b
b = matrix(c(5, 12, 17), ncol = 1)
cat("b=\n")
print(b)
```

E resolver para $\mathbf{x}$

```{r}
# Calculando o vetor x
x <- solve(A) %*% b

# Nomeando as linhas de x
rownames(x) <- c("x1", "x2", "x3")

# Exibindo o resultado
print(x)
```

A solução é $x_1=1$, $x_2=2$ e $x_3=2$.

### Aplicado sistemas de equações

Na análise de regressão será bastante comum fazer uso da representação de um sistema de $n$ equações simultâneas e $k$ variáveis, como é o exemplo a seguir:

$$
\begin{align*}
    y_1 &= x_{11} b_1 + x_{12} b_2 + \cdots + x_{1k} b_k \\
    y_2 &= x_{21} b_1 + x_{22} b_2 + \cdots + x_{2k} b_k \\
        &\vdots \\
    y_n &= x_{n1} b_1 + x_{n2} b_2 + \cdots + x_{nk} b_k
\end{align*}
$$

Usando a notação matricial, podemos escrever:

$$
\begin{bmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_n
\end{bmatrix}
=
\begin{bmatrix}
    x_{11} & x_{12} & \cdots & x_{1k} \\
    x_{21} & x_{22} & \cdots & x_{2k} \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{n1} & x_{n2} & \cdots & x_{nk}
\end{bmatrix}
\begin{bmatrix}
    b_1 \\
    b_2 \\
    \vdots \\
    b_k
\end{bmatrix}
$$

Mais sucintamente: $\mathbf{y} = \mathbf{Xb}$, onde

$$
\begin{equation*}
        \mathbf{y} = 
        \begin{bmatrix}
            y_1 \\ y_2 \\ \vdots \\ y_n
        \end{bmatrix}
        \quad ; \quad
        \mathbf{b} = 
        \begin{bmatrix}
            b_1 \\ b_2 \\ \vdots \\ b_k
        \end{bmatrix}
        \quad ; \quad
        \mathbf{x}_i = 
        \begin{bmatrix}
            x_{i1} \\ x_{i2} \\ \vdots \\ x_{ik}
        \end{bmatrix}
    \end{equation*}
$$

para $i = 1, 2, \dots, n$ (linhas da matriz $\mathbf{X}$).

A matriz $\mathbf{X}$ também pode ser escrita da seguinte forma:

$$
\begin{equation*}
\mathbf{X} = 
    \begin{bmatrix}
        x_{11} & x_{12} & \cdots & x_{1k} \\
        x_{21} & x_{22} & \cdots & x_{2k} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_{n1} & x_{n2} & \cdots & x_{nk}
    \end{bmatrix} = 
    \begin{bmatrix}
         \mathbf{x}_1' \\
         \mathbf{x}_2' \\
         \vdots \\
         \mathbf{x}_n'
    \end{bmatrix}
\end{equation*}
$$

Neste caso, cada $\mathbf{x}_i$ é um "vetor de covariáveis" para a $i$-ésima observação (cada linha de $\mathbf{X}$).

Retornando ao sistema original, podemos escrever cada equação individual usando vetores:

$$
\begin{align*}
    y_1 &= \mathbf{x}_1' \mathbf{b} \\
    y_2 &= \mathbf{x}_2' \mathbf{b} \\
        &\vdots \\
    y_n &= \mathbf{x}_n' \mathbf{b}
\end{align*}
$$

Cada vetor destes representa uma equação do tipo:

$$
y_i = \begin{bmatrix} x_{i1} & x_{i2} & \cdots & x_{ik} \end{bmatrix} 
      \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_k \end{bmatrix} = 
      x_{i1} b_1 + x_{i2} b_2 + \cdots + x_{ik} b_k
$$

### Algumas operações comuns

**Representando somatórios e o cálculo da média**

Denote por $\textbf{i}$ um vetor contendo uma coluna de uns.

$$
\textbf{i} = \begin{bmatrix} 1 \\ \vdots \\ 1 \end{bmatrix} 
$$

Podmeos representar uma soma por meio de um produto escalar entre $\textbf{i}$ e um vetor $\textbf{x}$:

$$
\textbf{i}'\textbf{x} = \begin{bmatrix} 1 & 1 & 1 \end{bmatrix} 
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} =
1x_1 + 1x_2 + \cdots + 1x_n = \sum_{i=1}^{n} x_i
$$

Para qualquer constante $a$ e vetor $\textbf{x}$, temos:

$$
    \sum_{i=1}^{n} a x_i = a \sum_{i=1}^{n} x_i = a \textbf{i}'\textbf{x}.
$$

Se $a = \frac{1}{n}$, obtemos a média aritmética:

$$
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i = \frac{1}{n} \textbf{i}'\textbf{x}
$$

**EXEMPLO**

```{r}
# Definindo o vetor x
x <- c(4, 5, 6, 3, 7)

# Criando um vetor de 1's com o mesmo comprimento de x
ones <- rep(1, length(x))

# Calculando a soma dos elementos de x usando o produto interno
soma_x <- t(ones) %*% x
soma_x <- as.numeric(soma_x)  # Convertendo para um número escalar
soma_x
```

Podemos calcular a média de

```{r}
# Calculando a média dos elementos de x
a <- 1 / length(x)
media_x <- a * (t(ones) %*% x) 
media_x
```

**Desvios em relação à média**

Uma matriz fundamental na estatística é a \textit{matriz de centralização}, usada para transformar dados em desvios em relação à sua média.

Para calcular os desvios em relação à média, primeiro precisamso definir um vetor com a média. Se $\bar{x} = \frac{1}{n} \textbf{i}'\textbf{x}$, que é uma constante, então:

$$
\textbf{i} \bar{x} =
\begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} \bar{x} = 
\begin{bmatrix} \bar{x} \\ \bar{x} \\ \vdots \\ \bar{x} \end{bmatrix}
$$

Uma vez que $\bar{x} = \frac{1}{n} \textbf{i}'\textbf{x}$, então $\textbf{i} \bar{x} =\frac{1}{n} \textbf{i} \textbf{i}'\textbf{x}$

Desvio em relação à média então podem ser escritos como:

$$
{\mathbf{x}} -  \frac{1}{n} \mathbf{i}\mathbf{i}'\mathbf{x} = 
\begin{bmatrix} 
x_1 - \bar{x} \\ 
x_2 - \bar{x} \\ 
\vdots \\ 
x_n - \bar{x} 
\end{bmatrix}  
$$

Considerando que $\mathbf{x} = \mathbf{I} \mathbf{x}$, temos que:

$$
\left[ \mathbf{I} \mathbf{x} - \frac{1}{n} \mathbf{i} \mathbf{i}' \mathbf{x} \right] = 
\left[ \mathbf{I} - \frac{1}{n} \mathbf{i} \mathbf{i}' \right] \mathbf{x} = \mathbf{M^0} \mathbf{x}
$$

**EXEMPLO**

```{r}
# Número de elementos em x
n <- length(x)

# Criando a matriz identidade I de dimensão n
I <- diag(n)

# Criando o vetor de 1's
ones <- matrix(1, n, 1)

# Construindo a matriz M0
M0 <- I - (1 / n) * (ones %*% t(ones))

# Calculando os desvios em relação à média
desvios <- M0 %*% x

# Imprimir resultados
cat("x =\n")
print(x)
cat("desvios de x em relação à média =\n")
print(round(desvios, 2))
```

## 2. Formas Linares e Formas Quadráticas

Considere a função linear:

$$f(x) = a_1x_1 + a_2x_2 + a_3x_3 + \ldots + a_nx_n$$

Podemos usar uma notação matricial para expressar essa função:

$$
f(\mathbf{x}) = \mathbf{a}'\mathbf{x}
$$

Dessa forma, temos:

$$
\mathbf{a}' =
\begin{bmatrix}
a_1 & a_2 & a_3 & \ldots & a_n
\end{bmatrix}
\qquad \mathrm{e} \qquad \mathbf{x}=
\begin{bmatrix}
x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n
\end{bmatrix}
$$

Considere agora, a função de duas variáveis $(x,y)$:

$$
f(x,y) = a_{11}x_1y_1 + a_{21}x_2y_1 + a_{31}x_3y_1 + a_{12}x_1y_2 + a_{22}x_2y_2 + a_{32}x_3y_2
$$

Na forma matricial temos uma **forma bilinear**:

$$
f(\mathbf{x},\mathbf{y}) = \mathbf{x}'\mathbf{A}\mathbf{y}
$$

em que:

$$
\mathbf{A} = \begin{bmatrix}
a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32}
\end{bmatrix}
$$

### Formas Quadráticas

Uma forma quadrática em $\mathbb{R}^n$ é uma função de valor real da forma

$$
Q(x_1, \ldots, x_n) = \sum_{i \leq j} a_{ij} x_i x_j
$$

Por exemplo, no $\mathbb{R}^2$ temos $Q(x_1, x_2) = a_{11}x_1^2 + a_{12}x_1x_2 + a_{22}x_2^2$.

Em termos matriciais, podemos pernsar na forma quadrática como uma forma bilinear em que $\mathbf{x}=\mathbf{y}$.

Em geral, se $\mathbf{A}$ é uma matriz simétrica $n \times n$ e $\mathbf{x}$ é um vetor-coluna $n \times 1$ de variáveis, então dizemos que a função

$$
Q_A(\mathbf{x}) = \mathbf{x}' \mathbf{A} \mathbf{x}
$$

é a forma quadrática associada com $A$.

**EXEMPLOS**

-   Exemplo 1.

$$
\mathbf{x}' \mathbf{A} \mathbf{x} = 
\begin{bmatrix} x_1 & x_2 \end{bmatrix}
\begin{bmatrix} a_1 & a_3 \\ a_3 & a_2 \end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 
a_1 x_1^2 + a_2 x_2^2 + 2 a_3 x_1 x_2 = 
$$

-   Exemplo 2.

$$
\mathbf{x}' \mathbf{A} \mathbf{x} = 
\begin{bmatrix} x_1 & x_2 & x_3 \end{bmatrix}
\begin{bmatrix} a_1 & a_4 & a_5 \\ a_4 & a_2 & a_6 \\ a_5 & a_6 & a_3 \end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = 
a_1 x_1^2 + a_2 x_2^2 + a_3 x_3^2 + 2 a_4 x_1 x_2 + 2 a_5 x_1 x_3 + 2 a_6 x_2 x_3
$$

-   Exemplo 3.

Com $\mathbf{A} = \begin{bmatrix} -3 & 5 \\ 4 & -2 \\ \end{bmatrix}$ e $\mathbf{x} = \begin{bmatrix} x_1 \\ x_2\end{bmatrix}$

$$
\begin{split}
\mathbf{x}^\intercal\mathbf{Ax} &= \begin{bmatrix}
x_1 & x_2\end{bmatrix}\begin{bmatrix}
-3 & 5  \\
4 & -2  \\
\end{bmatrix}\begin{bmatrix}
x_1 \\ x_2\end{bmatrix} \\[2ex]
&= \begin{bmatrix}
x_1 & x_2\end{bmatrix}\begin{bmatrix}
-3x_1 + 5x_2  \\
4x_1 -2x_2
\end{bmatrix} \\[2ex]
&= x_1(-3x_1 + 5x_2) + x_2(4x_1 -2x_2) \\[2ex]
&= -3x_1^2 + 5x_1x_2 + 4x_1x_2 -2x_2^2 \\[2ex]
&= -3x_1^2 + 9x_1x_2 -2x_2^2
\end{split}
$$

-   Exemplo 4.

Nos casos em que $\mathbf{A}$ é uma matriz diagonal, a forma quadrática $Q_A$ não tem termos mistos.

Por exemplo, se $\mathbf{A}$ é uma matriz identidade $n \times n$, então:

$$
Q_A(\mathbf{x}) = 
\mathbf{x}' I \mathbf{x} = 
\mathbf{x}' \mathbf{x} = 
\mathbf{x} \cdot \mathbf{x} = 
\|\mathbf{x}\|^2 = 
x_1^2 + x_2^2 + \cdots + x_n^2
$$

-   Exemplo 5.

Se $\mathbf{A}$ tem entradas diagonais $\lambda_1, \lambda_2, \ldots, \lambda_n$, então:

$$
Q_A(\mathbf{x}) = 
\mathbf{x}' A \mathbf{x} = 
\begin{bmatrix} 
x_1 & x_2 & \cdots & x_n 
\end{bmatrix}
\begin{bmatrix} 
\lambda_1 & 0 & \cdots & 0 \\ 
0 & \lambda_2 & \cdots & 0 \\ 
\vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & \cdots & \lambda_n 
\end{bmatrix}
\begin{bmatrix} 
x_1 \\ x_2 \\ \vdots \\ x_n 
\end{bmatrix} 
= \lambda_1 x_1^2 + \lambda_2 x_2^2 + \cdots + \lambda_n x_n^2
$$

### Aplicando formas quadráticas

Para uma única variável $\textbf{x}$, a soma dos quadrados dos desvios em relação à média é dada por:

$$
\sum_{i=1}^{n} (x_i - \bar{x})^2 = \left( \sum_{i=1}^{n} x_i^2 \right) - n \bar{x}^2.
$$

Em termos matriciais,

$$
\sum_{i=1}^{n} (x_i - \bar{x})^2 = 
(\textbf{x} - {\textbf{i}}\bar{x})' (\textbf{x} - {\textbf{i}}\bar{x}) = 
(\mathbf{M^0} \textbf{x})' (\mathbf{M^0} \textbf{x}) = 
\textbf{x}' \mathbf{M^0}' \mathbf{M^0} \textbf{x}
$$

A matriz $\mathbf{M^0}$ possui a propriedade de ser idempotente, ou seja, $\mathbf{M^0}' \mathbf{M^0} = \mathbf{M^0}$.

Assim: cterística distintiva de uma forma quadrática é o conjunto de valores que assume quando $\mathbf{x} \neq 0$. Queremos saber se $\mathbf{x} = 0$ é um máximo, mínimo ou nenhum dos dois.

Sob a ótica de um problema de otimização, isso sim é interessante!

Por exemplo, quando $\mathbf{x} \in \mathbb{R}$, isto é, a forma quadrática é $ax_1^2$:

-   Se $a > 0$ significa que $ax^2 \geq 0$ e é igual a 0 apenas quando $x = 0$. Tal forma é chamada **definida positiva**; $x = 0$ é um mínimo global.

-   Se $a < 0$ significa que $ax^2 \leq 0$ e é igual a 0 apenas quando $x = 0$. Tal forma é chamada **definida negativa**; $x = 0$ é um máximo global.

Uma matriz simétrica, $\mathbf{A}$, é chamada de definida positiva, semidefinida positiva, definida negativa, etc., de acordo com a forma quadrática correspondente $Q(\mathbf{x}) = \mathbf{x}' \mathbf{A} \mathbf{x}$.

Seja $\mathbf{A}$ uma matriz simétrica $n \times n$, então $\mathbf{A}$ é:

-   **definida positiva** se $\mathbf{x}' \mathbf{A} \mathbf{x} > 0$ para todo $\mathbf{x} \neq 0$ em $\mathbb{R}^n$;

-   **semidefinida positiva** se $\mathbf{x}' \mathbf{A} \mathbf{x} \geq 0$ para todo $\mathbf{x} \neq 0$ em $\mathbb{R}^n$;

-   **definida negativa** se $\mathbf{x}' \mathbf{A} \mathbf{x} < 0$ para todo $\mathbf{x} \neq 0$ em $\mathbb{R}^n$;

-   **semidefinida negativa** se $\mathbf{x}' \mathbf{A} \mathbf{x} \leq 0$ para todo $\mathbf{x} \neq 0$ em $\mathbb{R}^n$;

-   **indefinida** se $\mathbf{x}' \mathbf{A} \mathbf{x} > 0$ para algum $\mathbf{x} \neq 0$ em $\mathbb{R}^n$ e $< 0$ para outro $\mathbf{x}$ em $\mathbb{R}^n$.

## 3. Autovalores e autovetores

Os autovalores e autovetores são conceitos fundamentais em álgebra linear e têm ampla aplicação em econometria.

Dada uma matriz quadrada $\mathbf{A}$ de ordem $n \times n$, um número $\lambda$ é chamado de **autovalor** de $\mathbf{A}$ se existir um vetor não nulo $\mathbf{x}$ tal que:

$$
\mathbf{A} \mathbf{x} = \lambda \mathbf{x}
$$

Neste caso, o vetor $\mathbf{x}$ é chamado de **autovetor** associado ao autovalor $\lambda$.

Os autovalores ($\lambda$) indicam a magnitude do efeito de $\mathbf{A}$ sobre os vetores $\mathbf{x}$.

Por sua vez, os autovetores ($\mathbf{x}$) indicam as direções em que a aplicação da matriz $\mathbf{A}$ resulta em um escalonamento sem alteração da direção.

-   Número de autovalores: Uma matriz $n \times n$ terá até $n$ autovalores.

-   Determinante e traço: O produto dos autovalores de $\mathbf{A}$ é igual ao seu determinante, e a soma dos autovalores é igual ao traço de $\mathbf{A}$ (a soma dos elementos da diagonal).

-   Diagonalização: Se todos os autovalores de uma matriz são distintos e a matriz é simétrica, é possível diagonalizá-la, ou seja, encontrar uma matriz $\mathbf{P}$ de autovetores e uma matriz diagonal $\mathbf{\Lambda}$ de autovalores, tal que $\mathbf{A} = \mathbf{P} \mathbf{\Lambda} \mathbf{P}^{-1}$.

**EXEMPLO**

Vamos calcular os autovalores e autovetores de uma matriz $3 \times 3$ usando R:

Considere a matriz $\mathbf{A}$:

$$
\mathbf{A} = \begin{bmatrix} 3 & 1 \\ 1 & 2 \end{bmatrix}
$$

Para encontrar os autovalores, calculamos o polinômio característico de $\mathbf{A}$.

$$
\mathbf{A} - \lambda \mathbf{I} = 
\begin{bmatrix} 3 - \lambda & 1 \\ 
1 & 2 - \lambda
\end{bmatrix}
$$

Calculamos o determinante desta matriz e igualamos a zero:

$$
 \lambda^2 - 5 \lambda + 5 = 0
$$

As raízes dessa equação são:

$$
\lambda_1 = \frac{5 + \sqrt{5}}{2} \quad \text{e} \quad \lambda_2 = \frac{5 - \sqrt{5}}{2}
$$

Trata-se de uma parábola com concavidade 'para baixo' (côncava).

Vamos resolver isso no `R`:

```{r}
# Definindo uma matriz 3x3
A = matrix(c(3, 1,
             1, 2), nrow = 2, byrow = TRUE)
A
```

-   Autovalores

```{r}
# Calculando os autovalores e autovetores
eig_decomp = eigen(A)

# Extraindo autovalores
autovalores = eig_decomp$values
autovalores
```

Note que temos autovalores positivos.

-   Autovetores

```{r}
# Extraindo autovetores
autovetores = eig_decomp$vectors
autovetores
```

-   Verificando estas condições com base em:

\$\$ \begin{align*}
\mathbf{A} \mathbf{x} & = \lambda \mathbf{x} \\

\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} 
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} & = 
\lambda \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
\end{align*} \$\$

Vamos considerar o 1º autovalor $\lambda_1$ e o 1º autovetor

```{r}
# Verificando a propriedade: A * x = lambda * x para o primeiro autovalor e autovetor
lambda1 = autovalores[1]
x1 = autovetores[, 1]
cat("A.x =\n")
print(A %*% x1)
cat("lambda.x =\n")
print(lambda1 * x1)
```

**Autovalores, a definição da forma quadrática e otimização**

Seja $\mathbf{A}_{n \times n}$ uma matriz com autovalores $\lambda_1, \cdots, \lambda_n$.

-   Se $\mathbf{A}$ é **definida positiva**, então $\lambda_i > 0$ para $i = 1, \dots, n$.

-   Se $\mathbf{A}$ é **semidefinida positiva**, então $\lambda_i \geq 0$ para $i = 1, \dots, n$. O número de autovalores para os quais $\lambda_i > 0$ é igual ao posto de $\mathbf{A}$.

-   Se $\mathbf{A}$ é **definida negativa**, então $\lambda_i < 0$ para $i = 1, \dots, n$.

-   Se $\mathbf{A}$ é **semidefinida negativa**, então $\lambda_i \leq 0$ para $i = 1, \dots, n$. O número de autovalores para os quais $\lambda_i < 0$ é igual ao posto de $\mathbf{A}$.

-   Se $\mathbf{A}$ é **indefinida**, então $\mathbf{A}$ possui autovalores de sinais opostos, ou seja, existem $\lambda_i > 0$ para alguns $i$ e $\lambda_j < 0$ para outros $j$.

## 4. Diferenciação matricial

Sejam os vetores:

$$
\mathbf{a} = \begin{bmatrix} 
a_1 \\ a_2 \\ \vdots \\ a_n 
\end{bmatrix} \quad \text{e} \quad
\mathbf{x} = \begin{bmatrix} 
x_1 \\ x_2 \\ \vdots \\ x_n 
\end{bmatrix}
$$

Então:

$$
\mathbf{a}'\mathbf{x} = a_1 x_1 + a_2 x_2 + \cdots + a_n x_n
$$

A derivada de $\mathbf{a}'\mathbf{x}$ é dada por:

$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{x}} (\mathbf{a}' \mathbf{x}) &= 
\frac{\partial}{\partial \mathbf{x}} (a_1 x_1 + a_2 x_2 + \cdots + a_n x_n) \\
&= \begin{bmatrix}
\frac{\partial}{\partial x_1} (a_1 x_1 + a_2 x_2 + \cdots + a_n x_n) \\
\frac{\partial}{\partial x_2} (a_1 x_1 + a_2 x_2 + \cdots + a_n x_n) \\
\vdots \\
\frac{\partial}{\partial x_n} (a_1 x_1 + a_2 x_2 + \cdots + a_n x_n)
\end{bmatrix} = 
\begin{bmatrix} 
a_1 \\ a_2 \\ \vdots \\ a_n 
\end{bmatrix} \\
&= \mathbf{a}
\end{aligned}
$$

Considere agora a forma quadrática $\mathbf{x}' \mathbf{A} \mathbf{x}$

$$
\frac{\partial}{\partial \mathbf{x}} (\mathbf{x}' \mathbf{A} \mathbf{x}) = 2\mathbf{A} \mathbf{x}
$$

Vamos demostrar isso de uma forma um pouco diferente:

Considere a forma quadrática $\mathbf{x}' \mathbf{A} \mathbf{x}$, onde $\mathbf{A}$ é uma matriz simétrica.

$$
f(\mathbf{x}) = \mathbf{x}' \mathbf{A} \mathbf{x} = \sum_{i=1}^n \sum_{j=1}^n x_i a_{ij} x_j
$$

A derivada de $f(\mathbf{x})$ em relação a $\mathbf{x}$ é:

$$
\frac{\partial}{\partial \mathbf{x}} (\mathbf{x}' \mathbf{A} \mathbf{x}) = \frac{\partial}{\partial \mathbf{x}} \left( \sum_{i=1}^n \sum_{j=1}^n x_i a_{ij} x_j \right)
$$

Como estamos derivando em relação a cada $x_i$, temos:

$$
\frac{\partial}{\partial x_i} \left( \sum_{j=1}^n x_i a_{ij} x_j \right) = \sum_{j=1}^n a_{ij} x_j + \sum_{j=1}^n a_{ji} x_j = 2 \sum_{j=1}^n a_{ij} x_j
$$

Assim, a derivada é:

$$
\frac{\partial}{\partial \mathbf{x}} (\mathbf{x}' \mathbf{A} \mathbf{x}) = 2 \mathbf{A} \mathbf{x}
$$
